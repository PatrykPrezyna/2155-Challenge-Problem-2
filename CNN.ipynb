{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65478191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from utils_public import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "def plot_and_r2(preds_train, preds_test, ratings_train, ratings_test, advisor): \n",
    "    #Calculates \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.scatter(ratings_train, preds_train, label='Train Set Preds', s=3, c = \"#BBBBBB\") #train set in gray\n",
    "    plt.scatter(ratings_test, preds_test, label='Test Set Preds', s=5, c = \"#DC267F\") #test set in magenta\n",
    "    plt.plot([0,1], [0,1], label=\"Target\", linewidth=3, c=\"k\") #target line in black\n",
    "\n",
    "    #Set axis labels and title\n",
    "    plt.xlabel(\"Actual Rating\")\n",
    "    plt.ylabel(\"Predicted Rating\")\n",
    "    plt.title(f\"Advisor {advisor} Predictions\")\n",
    "\n",
    "    #Turn off top and right spines\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.legend() #Display legend\n",
    "    plt.show() #Show plot\n",
    "\n",
    "    #Calculate R2 score for train and test sets\n",
    "    print(f\"Advisor {advisor} Train Set R2 score: {r2_score(ratings_train, preds_train)}\") \n",
    "    print(f\"Advisor {advisor} Test Set R2 score: {r2_score(ratings_test, preds_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf78372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and run the model\n",
    "grids = load_grids()\n",
    "ratings = np.load(\"datasets/scores.npy\")\n",
    "ratings_df = pd.DataFrame(ratings, columns=[\"Wellness\", \"Tax\", \"Transportation\", \"Business\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aecb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num_iterations = 10  # how many new grids to create per original grid\n",
    "num_cell_to_replace = 1  # how many cells to modify in each new grid\n",
    "\n",
    "new_grids = []\n",
    "new_ratings = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    for grid in grids:\n",
    "        # Generate modified versions of the grid\n",
    "        new_grid = grid.copy()\n",
    "        for _ in range(num_cell_to_replace):\n",
    "            # Select random cell coordinates\n",
    "            x, y = random.randint(0, 6), random.randint(0, 6)\n",
    "            current_val = grid[x, y]\n",
    "            # Choose a different random value for replacement\n",
    "            new_val = random.choice([v for v in range(5) if v != current_val])\n",
    "            new_grid[x, y] = new_val\n",
    "        new_grids.append(new_grid)\n",
    "        new_ratings.append([np.nan] * 4)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "new_grids = np.array(new_grids)\n",
    "new_ratings = np.array(new_ratings)\n",
    "\n",
    "# Combine with existing data\n",
    "all_grids = np.vstack((grids, new_grids))\n",
    "all_ratings = np.vstack((ratings, new_ratings))\n",
    "\n",
    "# Remove duplicates\n",
    "unique_grids, unique_indices = np.unique(all_grids, axis=0, return_index=True)\n",
    "unique_ratings = all_ratings[unique_indices]\n",
    "\n",
    "print(f\"Number of unique grids: {unique_grids.shape[0]}\")\n",
    "\n",
    "# set to activate\n",
    "# grid = unique_grids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_predictions = [] #empty list to hold predictions\n",
    "histories = []  # Store training histories for each advisor\n",
    "for advisor in range(4): #loop over four advisors\n",
    "    print(f\"\\nTraining model for Advisor {advisor}\")\n",
    "    grids_subset, ratings_subset = select_rated_subset(grids, ratings[:,advisor]) #gets subset of the dataset \n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(grids_subset, ratings_subset, test_size=.2, random_state=42)\n",
    "\n",
    "    height, width = 7, 7\n",
    "    num_pixel_classes = 5  # pixels take values 0 to 4\n",
    "    # # One-hot encoding of categorical pixel values\n",
    "    X_train_encoded = tf.one_hot(X_train, depth=num_pixel_classes)  # Shape: (4000, 7, 7, 5)\n",
    "    X_validation_encoded = tf.one_hot(X_validation, depth=num_pixel_classes)  # Shape: (4000, 7, 7, 5)\n",
    "\n",
    "    # Build CNN model for regression\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(height, width, num_pixel_classes)),\n",
    "        layers.Conv2D(64, kernel_size=(2, 2), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(2, 2), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')  # Single output for regression\n",
    "    ])\n",
    "\n",
    "    # Compile model with mean squared error loss for regression\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model and capture history\n",
    "    history = model.fit(\n",
    "        X_train_encoded, \n",
    "        Y_train, \n",
    "        batch_size=32, \n",
    "        epochs=50, \n",
    "        validation_data=(X_validation_encoded, Y_validation),\n",
    "        verbose=1\n",
    "    )\n",
    "    histories.append(history)  # Store the training history\n",
    "\n",
    "    #predict\n",
    "    preds_train = model.predict(X_train_encoded) #predict on the train set\n",
    "    preds_test = model.predict(X_validation_encoded) #predict on the test set\n",
    "\n",
    "    plot_and_r2(preds_train, preds_test, Y_train, Y_validation, advisor)\n",
    "    \n",
    "    grids_encoded = tf.one_hot(unique_grids, depth=num_pixel_classes)  # Shape: (X, 7, 7, 5)\n",
    "    # Merge predictions with actual ratings\n",
    "    predictions = model.predict(grids_encoded) #predict on the train set\n",
    "\n",
    "    mask = np.where(~np.isnan(ratings[:,advisor])) #get the indices of the rated grids\n",
    "    predictions[mask, 0] = ratings[:, advisor][mask]  # assign to 2D predictions\n",
    "    all_predictions.append(predictions) #append predictions\n",
    "    all_predictions = [np.squeeze(a) for a in all_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves for all advisors\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot MSE Loss\n",
    "plt.subplot(2, 1, 1)\n",
    "for idx, history in enumerate(histories):\n",
    "    plt.plot(history.history['loss'], label=f'Advisor {idx} Training Loss (MSE)')\n",
    "    plt.plot(history.history['val_loss'], label=f'Advisor {idx} Validation Loss (MSE)', linestyle='--')\n",
    "plt.title('Model Loss (MSE) over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(2, 1, 2)\n",
    "for idx, history in enumerate(histories):\n",
    "    plt.plot(history.history['mae'], label=f'Advisor {idx} Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label=f'Advisor {idx} Validation MAE', linestyle='--')\n",
    "plt.title('Model MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics for each advisor\n",
    "for idx, history in enumerate(histories):\n",
    "    print(f\"\\nAdvisor {idx} final metrics:\")\n",
    "    print(f\"Training Loss (MSE): {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Validation Loss (MSE): {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"Validation MAE: {history.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.where(~np.isnan(ratings[:,advisor])) #get the indices of the rated grids\n",
    "# # predictions = predictions.reshape(-1, predictions.shape[0])\n",
    "# predictions[mask, 0] = ratings[:, advisor][mask]  # assign to 2D predictions\n",
    "# # predictions[mask] = ratings[:,advisor][mask] #replace the predictions with the actual ratings where available\n",
    "# all_predictions.append(predictions) #append predictions\n",
    "\n",
    "\n",
    "final_prediction_array = np.stack(all_predictions).T #stack the predictions\n",
    "min_predictions = np.min(final_prediction_array, axis=1) #minimum advisor score (as predicted)\n",
    "# print(f\"Number of valid grids (as predicted): {np.sum((min_predictions > 0.9) & (min_predictions < 1.1))}\") #number of valid grids (as predicted)\n",
    "print(f\"Number of valid grids (as predicted): {np.sum(min_predictions > 0.8)}\") #number of valid grids (as predicted)\n",
    "top_100_indices = np.argpartition(min_predictions, -100)[-100:] #indices of top 100 designs (as sorted by minimum advisor score)\n",
    "top_100_grids = unique_grids[top_100_indices] #get the top 100 grids\n",
    "print(f\"top_100_grids: {top_100_grids[:1]}\")\n",
    "\n",
    "#take the second best 100 from top 200\n",
    "top_500_indices = np.argpartition(min_predictions, -200)[-200:]  # indices of top 200 designs\n",
    "top_500_indices = top_500_indices[np.argsort(-min_predictions[top_500_indices])] #sort\n",
    "final_prediction_array[top_500_indices[:100]]\n",
    "top_100_grids = unique_grids[top_500_indices[100:200]] #get the top 100 grids\n",
    "top_500_grids = unique_grids[top_500_indices] #get the top 500 grids\n",
    "\n",
    "min_predictions_top5 = np.min(top_100_grids, axis=1) #minimum advisor score (as predicted)\n",
    "print(top_100_grids.shape)\n",
    "print(f\"top_100_grids: {top_100_grids[:1]}\")\n",
    "final_prediction_array[top_100_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grid_distance(grid1, grid2):\n",
    "    \"\"\"Compute the distance between two grids based on element-wise differences\"\"\"\n",
    "    return np.sum(grid1 != grid2)  # Count different elements\n",
    "\n",
    "def select_diverse_grids(grids, n_select=100):\n",
    "    \"\"\"\n",
    "    Select n_select most diverse grids from the input grids using a greedy approach.\n",
    "    Args:\n",
    "        grids: numpy array of shape (N, 7, 7) containing grid designs\n",
    "        n_select: number of grids to select\n",
    "    Returns:\n",
    "        numpy array of shape (n_select, 7, 7) containing selected diverse grids\n",
    "    \"\"\"\n",
    "    n_grids = len(grids)\n",
    "    \n",
    "    # Start with the first grid\n",
    "    selected_indices = [0]\n",
    "    remaining_indices = list(range(1, n_grids))\n",
    "    \n",
    "    # Greedy selection: at each step, select the grid that is most different from already selected ones\n",
    "    for _ in range(n_select - 1):\n",
    "        max_min_distance = -1\n",
    "        best_idx = -1\n",
    "        \n",
    "        # For each remaining grid\n",
    "        for idx in remaining_indices:\n",
    "            # Compute minimum distance to any selected grid\n",
    "            min_distance = float('inf')\n",
    "            for selected_idx in selected_indices:\n",
    "                distance = compute_grid_distance(grids[idx], grids[selected_idx])\n",
    "                min_distance = min(min_distance, distance)\n",
    "            \n",
    "            # Update best candidate if this grid has a larger minimum distance\n",
    "            if min_distance > max_min_distance:\n",
    "                max_min_distance = min_distance\n",
    "                best_idx = idx\n",
    "        \n",
    "        # Add the best grid to selected and remove from remaining\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_indices.remove(best_idx)\n",
    "    \n",
    "    return grids[selected_indices]\n",
    "\n",
    "# Select 100 most diverse grids from top 500\n",
    "diverse_grids = select_diverse_grids(top_500_grids, n_select=100)\n",
    "print(f\"Selected {len(diverse_grids)} diverse grids\")\n",
    "\n",
    "# Calculate average pairwise distance to verify diversity\n",
    "total_distance = 0\n",
    "n_pairs = 0\n",
    "for i in range(len(diverse_grids)):\n",
    "    for j in range(i + 1, len(diverse_grids)):\n",
    "        total_distance += compute_grid_distance(diverse_grids[i], diverse_grids[j])\n",
    "        n_pairs += 1\n",
    "\n",
    "avg_distance = total_distance / n_pairs\n",
    "print(f\"Average pairwise distance between selected grids: {avg_distance:.2f} different elements\")\n",
    "\n",
    "# Show shape of the selected grids\n",
    "print(f\"Shape of selected grids: {diverse_grids.shape}\")\n",
    "\n",
    "# Update top_100_grids with the diverse selection\n",
    "top_100_grids = diverse_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ratings_histogram(final_prediction_array, withmin=False) #plot histograms of top 100 designs\n",
    "score = diversity_score(top_100_grids)\n",
    "print(f\"Hypothetical diversity score if all top 100 grids are valid: {score:.4f} \")\n",
    "\n",
    "final_submission = top_100_grids.astype(int)\n",
    "assert final_submission.shape == (100, 7, 7)\n",
    "assert final_submission.dtype == int\n",
    "assert np.all(np.greater_equal(final_submission, 0) & np.less_equal(final_submission, 4))\n",
    "id = np.random.randint(1e8, 1e9-1)\n",
    "np.save(f\"results/{id}.npy\", final_submission)\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eff87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_grids(top_100_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "top 100\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 5\n",
    "Mean advisor rating (all advisors, all grids): 0.9596925926849548\n",
    "Score: 0.40858367346938773\n",
    "Submission ID: 61199621-5c5c-4f38-9122-f87615f8cc8e\n",
    "\n",
    "top second 100\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 11\n",
    "Mean advisor rating (all advisors, all grids): 0.9523805852710479\n",
    "Score: 0.3774\n",
    "Submission ID: 2254a970-e1ca-4bd5-9656-f13a36f728c5\n",
    "\n",
    "\n",
    "top third 100\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 9\n",
    "Mean advisor rating (all advisors, all grids): 0.9462318970556141\n",
    "Score: 0.4081102040816327\n",
    "Submission ID: be449510-b190-4fae-9387-16f19a83f4f7\n",
    "\n",
    "4\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 18\n",
    "Mean advisor rating (all advisors, all grids): 0.9378541694451326\n",
    "Score: 0.33855510204081635\n",
    "Submission ID: 76ad3233-0c75-45db-abb0-a27624250e0c\n",
    "\n",
    "5\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 16\n",
    "Mean advisor rating (all advisors, all grids): 0.9419590764106018\n",
    "Score: 0.34891836734693876\n",
    "Submission ID: 5d532188-06a8-4755-a2f1-adfec25858e2\n",
    "\n",
    "after the selection of diverse grids\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 18\n",
    "Mean advisor rating (all advisors, all grids): 0.9290512422197501\n",
    "Score: 0.3576530612244898\n",
    "Submission ID: 7cd8bcb4-52ac-4e53-b9d6-4523c52e22ba\n",
    "\n",
    "out of top 1000\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 26\n",
    "Mean advisor rating (all advisors, all grids): 0.9074144134730059\n",
    "Score: 0.29511020408163263\n",
    "Submission ID: 715da021-fee3-479c-8d89-965bbf283da7\n",
    "\n",
    "out of 200\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 9\n",
    "Mean advisor rating (all advisors, all grids): 0.9516056723356537\n",
    "Score: 0.4150122448979592\n",
    "Submission ID: 979efc73-bc95-45ac-a7e6-8f88f74cbcda\n",
    "\n",
    "\n",
    "out of 200\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 6\n",
    "Mean advisor rating (all advisors, all grids): 0.9580031518265091\n",
    "Score: 0.4419265306122449\n",
    "Submission ID: 7dc28e59-0b49-46e7-a0e0-27fbea16fe53\n",
    "\n",
    "best 100 after generation 1 different\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 3\n",
    "Mean advisor rating (all advisors, all grids): 0.9661947331155596\n",
    "Score: 0.45669795918367345\n",
    "Submission ID: 07d7122d-c2f4-47f1-b9d8-23782773fd83\n",
    "\n",
    "second 100 after generation 1 different\n",
    "Submission Successful!\n",
    "Name: Patryk&Angela\n",
    "Nickname: Patryk&Angela\n",
    "# Invalid grids: 7\n",
    "Mean advisor rating (all advisors, all grids): 0.9571057667558083\n",
    "Score: 0.4366204081632653\n",
    "Submission ID: 278bcd2a-561b-499f-bccc-e0c1ca8a24a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58131c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, X_train, Y_train, X_validation, Y_validation, num_epochs=10, batch_size=32, learning_rate=0.001, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Function to train a given model (CNN or DNN) using training and validation data.\n",
    "\n",
    "#     Args:\n",
    "#     - model: The model to train (e.g., CNN or DNN).\n",
    "#     - X_train: Training data (features).\n",
    "#     - Y_train: Training data (labels).\n",
    "#     - X_validation: Validation data (features).\n",
    "#     - Y_validation: Validation data (labels).\n",
    "#     - num_epochs: Number of training epochs (default: 10).\n",
    "#     - batch_size: Batch size for training (default: 32).\n",
    "#     - learning_rate: Learning rate for the optimizer (default: 0.001).\n",
    "#     - device: Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#     - train_losses: List of training losses for each epoch.\n",
    "#     - val_losses: List of validation losses for each epoch.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Move model to the specified device (GPU or CPU)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # Define loss function (Mean Squared Error for regression tasks)\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     # Define optimizer (Adam optimizer with specified learning rate)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # Create DataLoader for batching the training data\n",
    "#     train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32))\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     # Create DataLoader for batching the validation data\n",
    "#     validation_dataset = TensorDataset(torch.tensor(X_validation, dtype=torch.float32), torch.tensor(Y_validation, dtype=torch.float32))\n",
    "#     validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Lists to store train and validation losses\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         # Using tqdm for the training loop progress bar\n",
    "#         train_loader_tqdm = tqdm(train_loader, desc=f'Epoch [{epoch + 1}/{num_epochs}]', leave=False)\n",
    "\n",
    "#         # Iterate over batches of training data\n",
    "#         for inputs, labels in train_loader_tqdm:\n",
    "#             inputs = inputs.to(device)  # Move inputs to device (GPU/CPU)\n",
    "#             labels = labels.to(device)  # Move labels to device (GPU/CPU)\n",
    "\n",
    "#             # Zero the gradients before the next update\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass: compute model outputs\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # Compute the loss\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             # Backward pass: compute gradients\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Perform a single optimization step (update model parameters)\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Accumulate loss over the batch\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             # Update tqdm progress bar with current loss\n",
    "#             train_loader_tqdm.set_postfix({'Train Loss': running_loss / len(train_loader.dataset)})\n",
    "\n",
    "#         # Calculate average training loss for the epoch\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         train_losses.append(epoch_loss)\n",
    "\n",
    "#         # Validation loop (no gradient computation)\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in validation_loader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # Forward pass: compute model outputs\n",
    "#                 outputs = model(inputs)\n",
    "\n",
    "#                 # Compute validation loss\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#         # Calculate average validation loss for the epoch\n",
    "#         val_loss /= len(validation_loader.dataset)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "#         # Print the train and validation loss at the end of each epoch\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}] - Train MSE: {epoch_loss:.4f} - Validation MSE: {val_loss:.4f}')\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "\n",
    "#     # Return the recorded training and validation losses\n",
    "#     return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c62308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):  # Define all the layers here\n",
    "#         super(CNN, self).__init__()\n",
    "#         # All pooling will use 2x2 window and stride 2\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # First Conv layer: 1 input channel (grayscale), 4 output channels, kernel size 3x3, stride 1, padding 1\n",
    "#         self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Second Conv layer: 4 input channels, 8 output channels\n",
    "#         self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Third Conv layer: 8 input channels, 16 output channels\n",
    "#         self.conv3 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Fourth Conv layer: 16 input channels, 32 output channels\n",
    "#         self.conv4 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Operation to flatten the images into a vector\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(32 * 3 * 3, 16)  # Adjusted to match the output size after conv3\n",
    "#         self.fc2 = nn.Linear(16, 1)  # 5 output classes\n",
    "\n",
    "#     def forward(self, x):  # Call all the layers in this function\n",
    "#         # Reshape the input to ensure it has the correct channel dimension: [batch x 1 x 50 x 50]\n",
    "#         if len(x.shape) == 3:  # Input is [batch x 50 x 50]\n",
    "#             x = x.unsqueeze(1)  # Add channel dimension -> [batch x 1 x 50 x 50]\n",
    "\n",
    "#         # Convolve, then pass through ReLU, then pooling\n",
    "#         x = F.relu(self.conv1(x))  # [batch x 4 x 50 x 50]\n",
    "#         x = self.pool(x)  # [batch x 4 x 25 x 25]\n",
    "\n",
    "#         x = F.relu(self.conv2(x))  # [batch x 8 x 25 x 25]\n",
    "#         x = self.pool(x)  # [batch x 8 x 12 x 12]\n",
    "\n",
    "#         x = F.relu(self.conv3(x))  # [batch x 16 x 12 x 12]\n",
    "#         x = self.pool(x)  # [batch x 16 x 6 x 6]\n",
    "\n",
    "#         x = F.relu(self.conv4(x))  # [batch x 32 x 6 x 6]\n",
    "#         x = self.pool(x)  # [batch x 32 x 3 x 3]\n",
    "\n",
    "#         x = self.flatten(x)  # Flatten for the fully connected layer [batch x 288]\n",
    "\n",
    "#         # Pass through fully connected layers with ReLU\n",
    "#         x = F.relu(self.fc1(x))  # [batch x 16]\n",
    "\n",
    "#         # Final fully connected layer with no activation (since we are doing regression)\n",
    "#         x = self.fc2(x)  # Output layer with 5 outputs [batch x 5]\n",
    "#         return x\n",
    "\n",
    "# # Create a CNN model instance\n",
    "# model_cnn = CNN()\n",
    "# model_cnn.to(device)\n",
    "\n",
    "# # Print the model's architecture\n",
    "# summary(model_cnn, input_size=(1, 50, 50))  # Adjusted input size for grayscale images with 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aaf210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
