{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65478191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from utils_public import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "def plot_and_r2(preds_train, preds_test, ratings_train, ratings_test, advisor): \n",
    "    #Calculates \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.scatter(ratings_train, preds_train, label='Train Set Preds', s=3, c = \"#BBBBBB\") #train set in gray\n",
    "    plt.scatter(ratings_test, preds_test, label='Test Set Preds', s=5, c = \"#DC267F\") #test set in magenta\n",
    "    plt.plot([0,1], [0,1], label=\"Target\", linewidth=3, c=\"k\") #target line in black\n",
    "\n",
    "    #Set axis labels and title\n",
    "    plt.xlabel(\"Actual Rating\")\n",
    "    plt.ylabel(\"Predicted Rating\")\n",
    "    plt.title(f\"Advisor {advisor} Predictions\")\n",
    "\n",
    "    #Turn off top and right spines\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.legend() #Display legend\n",
    "    plt.show() #Show plot\n",
    "\n",
    "    #Calculate R2 score for train and test sets\n",
    "    print(f\"Advisor {advisor} Train Set R2 score: {r2_score(ratings_train, preds_train)}\") \n",
    "    print(f\"Advisor {advisor} Test Set R2 score: {r2_score(ratings_test, preds_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf78372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and run the model\n",
    "grids = load_grids()\n",
    "ratings = np.load(\"datasets/scores.npy\")\n",
    "ratings_df = pd.DataFrame(ratings, columns=[\"Wellness\", \"Tax\", \"Transportation\", \"Business\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_predictions = [] #empty list to hold predictions\n",
    "histories = []  # Store training histories for each advisor\n",
    "for advisor in range(4): #loop over four advisors\n",
    "    print(f\"\\nTraining model for Advisor {advisor}\")\n",
    "    grids_subset, ratings_subset = select_rated_subset(grids, ratings[:,advisor]) #gets subset of the dataset \n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(grids_subset, ratings_subset, test_size=.2, random_state=42)\n",
    "\n",
    "    height, width = 7, 7\n",
    "    num_pixel_classes = 5  # pixels take values 0 to 4\n",
    "    # # One-hot encoding of categorical pixel values\n",
    "    X_train_encoded = tf.one_hot(X_train, depth=num_pixel_classes)  # Shape: (4000, 7, 7, 5)\n",
    "    X_validation_encoded = tf.one_hot(X_validation, depth=num_pixel_classes)  # Shape: (4000, 7, 7, 5)\n",
    "\n",
    "    # Build CNN model for regression\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(height, width, num_pixel_classes)),\n",
    "        layers.Conv2D(64, kernel_size=(2, 2), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(2, 2), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')  # Single output for regression\n",
    "    ])\n",
    "\n",
    "    # Compile model with mean squared error loss for regression\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model and capture history\n",
    "    history = model.fit(\n",
    "        X_train_encoded, \n",
    "        Y_train, \n",
    "        batch_size=32, \n",
    "        epochs=50, \n",
    "        validation_data=(X_validation_encoded, Y_validation),\n",
    "        verbose=1\n",
    "    )\n",
    "    histories.append(history)  # Store the training history\n",
    "\n",
    "    #predict\n",
    "    preds_train = model.predict(X_train_encoded) #predict on the train set\n",
    "    preds_test = model.predict(X_validation_encoded) #predict on the test set\n",
    "\n",
    "    plot_and_r2(preds_train, preds_test, Y_train, Y_validation, advisor)\n",
    "    \n",
    "    grids_encoded = tf.one_hot(grids, depth=num_pixel_classes)  # Shape: (4000, 7, 7, 5)\n",
    "    # Merge predictions with actual ratings\n",
    "    predictions = model.predict(grids_encoded) #predict on the train set\n",
    "\n",
    "    mask = np.where(~np.isnan(ratings[:,advisor])) #get the indices of the rated grids\n",
    "    predictions[mask, 0] = ratings[:, advisor][mask]  # assign to 2D predictions\n",
    "    all_predictions.append(predictions) #append predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.where(~np.isnan(ratings[:,advisor])) #get the indices of the rated grids\n",
    "# predictions = predictions.reshape(-1, predictions.shape[0])\n",
    "predictions[mask, 0] = ratings[:, advisor][mask]  # assign to 2D predictions\n",
    "# predictions[mask] = ratings[:,advisor][mask] #replace the predictions with the actual ratings where available\n",
    "all_predictions.append(predictions) #append predictions\n",
    "all_predictions = [np.squeeze(a) for a in all_predictions]\n",
    "\n",
    "final_prediction_array = np.stack(all_predictions).T #stack the predictions\n",
    "min_predictions = np.min(final_prediction_array, axis=1) #minimum advisor score (as predicted)\n",
    "# print(f\"Number of valid grids (as predicted): {np.sum((min_predictions > 0.9) & (min_predictions < 1.1))}\") #number of valid grids (as predicted)\n",
    "print(f\"Number of valid grids (as predicted): {np.sum(min_predictions > 0.75)}\") #number of valid grids (as predicted)\n",
    "top_100_indices = np.argpartition(min_predictions, -100)[-100:] #indices of top 100 designs (as sorted by minimum advisor score)\n",
    "top_100_grids = grids[top_100_indices] #get the top 100 grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32869225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves for all advisors\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot MSE Loss\n",
    "plt.subplot(2, 1, 1)\n",
    "for idx, history in enumerate(histories):\n",
    "    plt.plot(history.history['loss'], label=f'Advisor {idx} Training Loss (MSE)')\n",
    "    plt.plot(history.history['val_loss'], label=f'Advisor {idx} Validation Loss (MSE)', linestyle='--')\n",
    "plt.title('Model Loss (MSE) over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(2, 1, 2)\n",
    "for idx, history in enumerate(histories):\n",
    "    plt.plot(history.history['mae'], label=f'Advisor {idx} Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label=f'Advisor {idx} Validation MAE', linestyle='--')\n",
    "plt.title('Model MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics for each advisor\n",
    "for idx, history in enumerate(histories):\n",
    "    print(f\"\\nAdvisor {idx} final metrics:\")\n",
    "    print(f\"Training Loss (MSE): {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Validation Loss (MSE): {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"Validation MAE: {history.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ratings_histogram(final_prediction_array, withmin=False) #plot histograms of top 100 designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba806a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = diversity_score(top_100_grids)\n",
    "print(f\"Hypothetical diversity score if all top 100 grids are valid: {score:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a371f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = grids[top_100_indices].astype(int)\n",
    "assert final_submission.shape == (100, 7, 7)\n",
    "assert final_submission.dtype == int\n",
    "assert np.all(np.greater_equal(final_submission, 0) & np.less_equal(final_submission, 4))\n",
    "id = np.random.randint(1e8, 1e9-1)\n",
    "np.save(f\"results/{id}.npy\", final_submission)\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58131c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, X_train, Y_train, X_validation, Y_validation, num_epochs=10, batch_size=32, learning_rate=0.001, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Function to train a given model (CNN or DNN) using training and validation data.\n",
    "\n",
    "#     Args:\n",
    "#     - model: The model to train (e.g., CNN or DNN).\n",
    "#     - X_train: Training data (features).\n",
    "#     - Y_train: Training data (labels).\n",
    "#     - X_validation: Validation data (features).\n",
    "#     - Y_validation: Validation data (labels).\n",
    "#     - num_epochs: Number of training epochs (default: 10).\n",
    "#     - batch_size: Batch size for training (default: 32).\n",
    "#     - learning_rate: Learning rate for the optimizer (default: 0.001).\n",
    "#     - device: Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#     - train_losses: List of training losses for each epoch.\n",
    "#     - val_losses: List of validation losses for each epoch.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Move model to the specified device (GPU or CPU)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # Define loss function (Mean Squared Error for regression tasks)\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     # Define optimizer (Adam optimizer with specified learning rate)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # Create DataLoader for batching the training data\n",
    "#     train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32))\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     # Create DataLoader for batching the validation data\n",
    "#     validation_dataset = TensorDataset(torch.tensor(X_validation, dtype=torch.float32), torch.tensor(Y_validation, dtype=torch.float32))\n",
    "#     validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Lists to store train and validation losses\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         # Using tqdm for the training loop progress bar\n",
    "#         train_loader_tqdm = tqdm(train_loader, desc=f'Epoch [{epoch + 1}/{num_epochs}]', leave=False)\n",
    "\n",
    "#         # Iterate over batches of training data\n",
    "#         for inputs, labels in train_loader_tqdm:\n",
    "#             inputs = inputs.to(device)  # Move inputs to device (GPU/CPU)\n",
    "#             labels = labels.to(device)  # Move labels to device (GPU/CPU)\n",
    "\n",
    "#             # Zero the gradients before the next update\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass: compute model outputs\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # Compute the loss\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             # Backward pass: compute gradients\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Perform a single optimization step (update model parameters)\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Accumulate loss over the batch\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             # Update tqdm progress bar with current loss\n",
    "#             train_loader_tqdm.set_postfix({'Train Loss': running_loss / len(train_loader.dataset)})\n",
    "\n",
    "#         # Calculate average training loss for the epoch\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         train_losses.append(epoch_loss)\n",
    "\n",
    "#         # Validation loop (no gradient computation)\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in validation_loader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # Forward pass: compute model outputs\n",
    "#                 outputs = model(inputs)\n",
    "\n",
    "#                 # Compute validation loss\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#         # Calculate average validation loss for the epoch\n",
    "#         val_loss /= len(validation_loader.dataset)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "#         # Print the train and validation loss at the end of each epoch\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}] - Train MSE: {epoch_loss:.4f} - Validation MSE: {val_loss:.4f}')\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "\n",
    "#     # Return the recorded training and validation losses\n",
    "#     return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c62308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):  # Define all the layers here\n",
    "#         super(CNN, self).__init__()\n",
    "#         # All pooling will use 2x2 window and stride 2\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # First Conv layer: 1 input channel (grayscale), 4 output channels, kernel size 3x3, stride 1, padding 1\n",
    "#         self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Second Conv layer: 4 input channels, 8 output channels\n",
    "#         self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Third Conv layer: 8 input channels, 16 output channels\n",
    "#         self.conv3 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Fourth Conv layer: 16 input channels, 32 output channels\n",
    "#         self.conv4 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Operation to flatten the images into a vector\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(32 * 3 * 3, 16)  # Adjusted to match the output size after conv3\n",
    "#         self.fc2 = nn.Linear(16, 1)  # 5 output classes\n",
    "\n",
    "#     def forward(self, x):  # Call all the layers in this function\n",
    "#         # Reshape the input to ensure it has the correct channel dimension: [batch x 1 x 50 x 50]\n",
    "#         if len(x.shape) == 3:  # Input is [batch x 50 x 50]\n",
    "#             x = x.unsqueeze(1)  # Add channel dimension -> [batch x 1 x 50 x 50]\n",
    "\n",
    "#         # Convolve, then pass through ReLU, then pooling\n",
    "#         x = F.relu(self.conv1(x))  # [batch x 4 x 50 x 50]\n",
    "#         x = self.pool(x)  # [batch x 4 x 25 x 25]\n",
    "\n",
    "#         x = F.relu(self.conv2(x))  # [batch x 8 x 25 x 25]\n",
    "#         x = self.pool(x)  # [batch x 8 x 12 x 12]\n",
    "\n",
    "#         x = F.relu(self.conv3(x))  # [batch x 16 x 12 x 12]\n",
    "#         x = self.pool(x)  # [batch x 16 x 6 x 6]\n",
    "\n",
    "#         x = F.relu(self.conv4(x))  # [batch x 32 x 6 x 6]\n",
    "#         x = self.pool(x)  # [batch x 32 x 3 x 3]\n",
    "\n",
    "#         x = self.flatten(x)  # Flatten for the fully connected layer [batch x 288]\n",
    "\n",
    "#         # Pass through fully connected layers with ReLU\n",
    "#         x = F.relu(self.fc1(x))  # [batch x 16]\n",
    "\n",
    "#         # Final fully connected layer with no activation (since we are doing regression)\n",
    "#         x = self.fc2(x)  # Output layer with 5 outputs [batch x 5]\n",
    "#         return x\n",
    "\n",
    "# # Create a CNN model instance\n",
    "# model_cnn = CNN()\n",
    "# model_cnn.to(device)\n",
    "\n",
    "# # Print the model's architecture\n",
    "# summary(model_cnn, input_size=(1, 50, 50))  # Adjusted input size for grayscale images with 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aaf210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
